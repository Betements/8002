{
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat_minor": 2,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Classes File",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "'''\nDo's\n- look to replicate a regression from Chetty Friedman Atlas of Opportunity\n- rubenstein\n'''",
      "metadata": {
        "trusted": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "\"\\nDo's\\n- look to replicate a regression from Chetty Friedman Atlas of Opportunity\\n- rubenstein\\n\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "raw",
      "source": "More Practical Methods to Reduce Privacy Loss when Disclosing Statistics Based on Microdata\n\nAbstract:\n\nWe review and extend the noise adding algorithm proposed by R.Chetty and J.Friedman (2019). We revisit the existing literature on differential privacy to propose more concrete solutions for Social Scientists confronted with the problem of publicly releasing statistics that could disclose confidential information. We concern ourselves with the release of differential private regression estimates and its challenge of calculating sensitivity. First we evaluate the Chetty, Friedman algorithm against data from the U.S. Census Tract, with attention to the accuracy of the noise-infused estimates to the true estimate. We compare OLS, MM, and SMDM estimation techniques for both the Chetty algorithm and the Dwork Lei algorithm from ten years prior. We evaluate B. Rubenstein's Sensitivity Sampler which circumvents some of the problems of regression sensitivity analysis.\nWe extend the univariate regression problem to a bivariate problem.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Local Import\nimport numpy as np\nimport pandas as pd\nimport math\nimport scipy.stats as ss\nimport numpy.linalg as la\nfrom itertools import product\nimport numpy.random as nr\nimport pickle\n\n# Plot Tools\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom matplotlib import cycler\ncolors = cycler('color', \n       ['#EE6666', '#3388BB', '#9988DD', '#EECC55', \n       '#88BB44', '#FFBBBB'])\n\nplt.rc('axes', facecolor='#E6E6E6', edgecolor='none',\n      axisbelow=True, grid=True, prop_cycle=colors)\nplt.rc('grid', color='w', linestyle='solid')\nplt.rc('xtick', direction='out', color='gray')\nplt.rc('ytick', direction='out', color='gray')\nplt.rc('patch', edgecolor='#E6E6E6')\nplt.rc('lines', linewidth=2)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# R in Python Interface\nimport rpy2\nimport rpy2.robjects as ro\nfrom rpy2.robjects.packages import importr\nimport rpy2.robjects.numpy2ri as rnp\nrobustbase = importr('robustbase')\nbase = importr('base')\nutils = importr('utils')\ndiffpriv = importr('diffpriv')",
      "metadata": {
        "trusted": true
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Run this in an R console if it ever disappears\n# install.packages('diffpriv')\n# install.packages(\"devtools\")\n# devtools::install_github(\"brubinstein/diffpriv\")\n\n## a target function we'd like to run on private data X, releasing the result\ntarget <- function(X) mean(X)\n\n## target seeks to release a numeric, so we'll use the Laplace mechanism---a\n## standard generic mechanism for privatizing numeric responses\nlibrary(diffpriv)\nmech <- DPMechLaplace(target = target)\n\n'''\nTo run mech on a dataset X we must first determine the sensitivity of target to small changes to input dataset. \nOne avenue is to analytically bound sensitivity (on paper; see the vignette) \nand supply it via the sensitivity argument of mechanism construction: in this case not hard if we assume bounded data, \nbut in general sensitivity can be very non-trivial to calculate manually. The other approach, which we follow in this example, \nis sensitivity sampling: repeated probing of target to estimate sensitivity automatically. \nWe need only specify a distribution for generating random probe datasets; sensitivitySampler() takes care of the rest. \nThe price we pay for this convenience is the weaker form of random differential privacy.\n'''\n\n## set a dataset sampling distribution, then estimate target sensitivity with\n## sufficient samples for subsequent mechanism responses to achieve random\n## differential privacy with confidence 1-gamma\ndistr <- function(n) rnorm(n)\nmech <- sensitivitySampler(mech, oracle = distr, n = 5, gamma = 0.1)\n#> Sampling sensitivity with m=285 gamma=0.1 k=285\nmech@sensitivity    \n## DPMech and subclasses are S4: slots accessed via @\n#> [1] 0.8089517\n\nX <- c(0.328,-1.444,-0.511,0.154,-2.062) # length is sensitivitySampler() n\nr <- releaseResponse(mech, privacyParams = DPParamsEps(epsilon = 1), X = X)\ncat(\"Private response r$response:   \", r$response,\n  \"\\nNon-private response target(X):\", target(X))\n#> Private response r$response:    -1.119506 \n#> Non-private response target(X): -0.707",
      "metadata": {
        "trusted": true
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Test Data - Census Tract\n%cd '/home/nbuser/library/example_code_implementation_guide/'\nstata = pd.read_stata('private_data_by_cells.dta')\ndata = stata[stata.columns[::-1]]",
      "metadata": {
        "trusted": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "text": "/home/nbuser/library/example_code_implementation_guide\n",
          "name": "stdout",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "raw",
      "source": "MSE = []\n\nfor i in range(1,11):\n    noise_θ = []\n    diff = []\n    \n    # But also, this needs to be done 500 times\n    for j in range(0,500):\n        # Draws random samples from Laplace or Normal:\n        ω = np.random.laplace(0, 1 / np.sqrt(2))\n        \n        # Noise infused Statistics\n        noise_θ = [a + np.sqrt(2) * (χ / (i * b)) * ω for (a, b) in zip(θ, N)]\n        diff.append(np.square(np.subtract(noise_θ, θ)))\n        \n    # Compute MSE\n    MSE.append(np.mean(diff))",
      "metadata": {
        "trusted": true
      }
    },
    {
      "cell_type": "code",
      "source": "def npl(x):\n    return np.asarray([i.tolist() for i in x])\n\nclass Wrangle:\n    '''\n    Data Wrangling Class'''\n    def __init__(self, data, aggregate=False, partition=False):\n        # Numpy-nize Census Tract\n        self.Key = data.keys()\n        if aggregate:\n            Y = np.asarray(data.loc[:, self.Key[1]])\n            X = np.asarray(data.loc[:, self.Key[2]])\n            self.N = len(X)\n        elif partition:\n            None\n        else:\n            if len(self.Key) > 3:\n                None\n            else:\n                Cell = np.unique(np.array(data[self.Key[0]])).astype(int)\n                Y = [np.asarray(data.loc[data.loc[:, self.Key[0]]== i, self.Key[1]]) for i in Cell]\n                X = [np.asarray(data.loc[data.loc[:, self.Key[0]]== i, self.Key[2]]) for i in Cell]\n                self.N = np.asarray([len(i) for i in X])\n        self.X = X\n        self.Y = Y \n        self.v = len(self.Key) - 1\n                \n    def __call__(self):\n        return self.X, self.Y, self.N, self.v\n\nclass Methods:\n    ''' Class of regression methods and their parts\n    '''\n    def __init__(self, X, Y, N, v, method=None):\n        ''' Make sure you are using at most 2-dimensional arrays for X, Y\n        '''\n        # We assume X is organised by index with dependent variables inside each\n        self.X = np.asarray(X)\n        self.Y = np.asarray(Y)\n        self.method = method\n        if self.method == \"SMDM\":\n            self.form = ro.Formula(\"y~x\")\n        self.N = N\n        self.v = v\n        self.o = np.asarray([list(i) for i in product([0, 1], repeat = self.v)])\n        if type(self.N) is int:\n            self.index = [None]\n        else:\n            self.index = range(len(self.N))\n        self.range = range(4) \n        self.nd75 = ss.norm.ppf(0.75)\n    \n    def sort(self, I=None, O=None):\n        ''' Sort arrays and append outlier variations\n        '''\n        if I is None:\n            y = self.Y\n            x = self.X\n        else:\n            y = self.Y[I]\n            x = self.X[I]\n        if O is not None:\n            y = np.block([y, self.o[O][0]])\n            x = np.block([x, self.o[O][1]])\n        return x, y\n    \n    def stack(self, x):\n        ''' Add constant to estimate \n        '''\n        return np.block([[np.ones(len(x))], [x.T]]).T\n    \n    def lstsq(self, x, y, w=None, se=False):\n        ''' Calculus linear least-squares\n        '''\n        x = self.stack(x)\n        if w is not None:\n            X = x.T.dot(np.diag(w)).T\n        else:\n            X = x\n        β̂ = np.dot(la.inv(X.T.dot(x)), X.T.dot(y))\n        r = y - np.dot(X, β̂)\n        if se:\n            se = np.sqrt(r.dot(r) / np.sum(np.square(X.T[1] - np.mean(X.T[1]))) / (len(r) - self.v))\n        return β̂, r, se\n\n    # OLS Method:\n    def OLS(self, I=None, O=None, se=False):\n        x, y = self.sort(I=I, O=O)\n        β̂, r, se = self.lstsq(x, y, None, se)\n        return β̂, r, se\n        \n    # MM Method:\n    def MAD(self, r):\n        ''' Median Absolute Deviation \n        '''\n        return np.median(np.absolute(r - np.median(r)))\n    \n    def Tukey(self, u, c=1.547):\n        ''' Tukey's biweight function ''' \n        ρ = np.power(u, 2) / 2 - np.power(u, 4) / (2 * math.pow(c, 2)) + np.power(u, 6) / (6 * math.pow(c, 4))\n        if np.any(np.abs(u) > c):\n            ρ[np.abs(u) > c] = math.pow(c, 2) / 6            \n        return ρ\n            \n    def σ(self, r, w=None, K=0.199):\n        ''' The scale for the S-Estimator we wish to minimize\n        '''\n        if w is None:\n            σ = self.MAD(r) / self.nd75 \n        else:\n            σ = np.sqrt(np.sum(np.multiply(np.square(r),w)) / (len(r) * K))\n        return σ\n    \n    def u(self, r, σ):\n        ''' Bisquare ratio \n        '''\n        return r / σ\n    \n    def weight(self, u, method=\"S\", it=False):\n        ''' Estimator Reweighing\n        '''\n        if method==\"S\":\n            if not it:\n            # iteration = 1\n                w = np.square(u / 1.547)\n                if np.any(np.abs(u) > 1.547):\n                    w[np.abs(u) > 1.547] = 0\n                weight = np.square(1 - w)\n            else:\n                weight = self.Tukey(u) / np.square(u)\n        if method==\"MM\":\n            w = np.square(u / 4.685)\n            if np.any(np.abs(u) > 4.685):\n                w[np.abs(u) > 4.685] = 0\n            weight = np.square(1 - w)\n        return weight\n\n    def MM(self, I=None, O=None, se=False, maxiter=100):\n        ''' MM-Estimator regression algorithm as specified in Susanti 2009\n        '''\n        x, y = self.sort(I, O)\n        β̂, r, se = self.lstsq(x, y, None, False)\n        # 2. S-Estimation\n        # 1st iteration\n        σ = self.σ(r)\n        u = self.u(r, σ)\n        w = self.weight(u, \"S\", False)\n        β̂, r, se = self.lstsq(x, y, w, False)\n        # IRWLS\n        for i in range(maxiter):\n            σ = self.σ(r, w)\n            u = self.u(r, σ)\n            w = self.weight(u, \"S\", True)\n            S, r, se = self.lstsq(x, y, w, False)\n            if np.allclose(S, β̂, rtol=1e-09):\n                break\n            else:\n                β̂ = S\n        # We use the MAD of the residuals of the S-estimator\n        σ = self.σ(r)\n        # 3. MM-Estimate Convergence \n        for i in range(maxiter):\n            u = self.u(r, σ)\n            w = self.weight(u, \"MM\")\n            M, r, se = self.lstsq(x, y, w, False)\n            if np.allclose(M, β̂, rtol=1e-03):\n                break\n            else:\n                β̂ = M\n        if se:\n            β̂s, rs, se = self.lstsq(x, y, w, True)\n        return M, r, se, σ   \n        \n    # SMDM Method:\n    def SMDM(self, I=None, O=None):\n        ''' Run SMDM from R's 'lmrob' package via rpy2\n        '''\n        x, y = self.sort(I, O)\n        try:\n            nr, nc = x.shape\n        except:\n            nr = x.shape[0]\n            nc = 1\n        # Set up formula environment\n        rnp.activate()\n        form.environment[\"y\"] = ro.r['matrix'](y, nrow=nr, ncol=1)\n        form.environment[\"x\"] = ro.r['matrix'](x, nrow=nr, ncol=nc)\n        # Run Robust Regression\n        lmr = robustbase.lmrob(self.form, method = \"SMDM\", setting=\"KS2014\")\n        # must copy variables because of memory constraint\n        betas = np.array(lmr.rx2(\"coefficients\"), copy=True)\n        scale = np.array(lmr.rx2(\"scale\"), copy=True)\n        residuals = np.array(lmr.rx2(\"residuals\"), copy=True)\n        stderr = np.array(np.diag(lmr.rx2(\"cov\")), copy=True)\n        return betas, residuals, stderr, scale\n    \n    def __call__(self, withoutliers=True):\n        ''' Specify regression method and run all regression in one/two line(s)\n        '''\n        if self.method == \"OLS\":\n            if self.index is None:\n                regression = self.OLS(None, None, False)\n                if withoutliers:\n                    withoutliers = np.asarray([self.OLS(None, O, False) for O in self.range])\n            else:\n                regression = np.asarray([self.OLS(I, None, False) for I in self.index])\n                if withoutliers:\n                    withoutliers = np.asarray([[self.OLS(I, O, False) for O in self.range] for I in self.index])    \n        elif self.method == \"MM\":\n            if self.index is None:\n                regression = self.MM(None, None, False, 100)\n                if withoutliers:\n                    withoutliers = np.asarray([self.MM(None, O, False) for O in self.range])\n            else:\n                regression = np.asarray([self.MM(I, None, False) for I in self.index])\n                if withoutliers:\n                    withoutliers = np.asarray([[self.MM(I, O, False) for O in self.range] for I in self.index])\n        elif self.method == \"SMDM\":\n            if self.index is None:\n                regression = self.SMDM(None, None)\n                if withoutliers:\n                    withoutliers = np.asarray([self.SMDM(None, O) for O in self.range])\n            else:\n                regression = np.asarray([self.SMDM(I, None) for I in self.index])\n                if withoutliers:\n                    withoutliers = np.asarray([[self.MM(I, O) for O in self.range] for I in self.index])\n        return regression, withoutliers\n\nclass LocSnoises:\n    ''' Calculates Local Sensivity and releases statistics a la Chetty & Friedman\n    '''\n    def __init__(self, regression, withoutliers, N):\n        self.RA = regression\n        self.RB = withoutliers\n        self.range = range(4)\n        self.N = N\n        if type(self.N) is int:\n            self.index = [None]\n        else:\n            self.index = range(len(self.N))\n            \n    def ω(self):\n        ''' draws from Laplace Distribution \n        '''\n        return nr.laplace(0, 1/math.sqrt(2))\n    def LSR(self):\n        self.oR()\n        ''' \n        Calculate Local Sensitivity \n        '''\n        MOSE = max(np.multiply(self.N, \n                [max([abs(self.RB[i][j][0][1] - self.RA[i][0][1]) for j in self.range]) for i in self.index]))\n        return np.transpose([[self.RA[i][j] for j in [0, 2]] for i in self.index]), MOSE\n    \n    def noise(self, θseθ, χ):\n        ''' \n        Add Noise to Statistics \n        '''\n        NN = np.asarray(self.D.N).astype(float)\n        S = NN * self.ϵ\n        noise = lambda x: x * math.sqrt(2)\n        \n        nθ = [i[1] for i in θseθ[0]] + χ * noise(self.ω()) / S\n        sen = np.square(θseθ[1]) + 2 * np.square(χ / S)\n        senθ = np.sqrt(sen.astype(float))\n        nsenθ = senθ + χ * noise(self.ω()) / S\n        nsenθ = np.asarray([\"Sample Size Too Small\" if i==np.inf else i for i in nsenθ])\n        nN = self.D.N * (1 + noise(self.ω()) / S)\n        return nθ, nsenθ, nN\n    \n    def __call__(self):\n        ''' \n        Release Noise Infused Statistics \n        '''\n        m1, m2 = self.LSR()\n        nθ, nsenθ, nN = self.noise(m1, m2)\n        return nθ, nsenθ, nN\n    \nclass DWORK:\n    def __init__(self, Tr, r, o, ϵ, psize):\n        ''' We have assumed aggregate data instead of tract partitioning\n            ### b is org by cells with b's in cols\n            ### B.T is B0,B1 arrays\n            ### db is org by cell with alt's within - 3dim\n            ### dB.T splits B_0 and B_1 alts into two arrays with 4 subarrys each\n        '''\n        # length of each cell\n        self.n = psize\n        # base for each cell (for S algorithm)\n        self.base = np.asarray([1 + 1 / np.log(i) for i in self.n])\n        \n        # store β̂'s and β̂ᵢ's:\n        self.Tb = Tr[0][0][0]\n        self.b = npl(r.T[0]).T\n        self.db = npl(o.T[0].T)\n        \n        # number of partitions range\n        self.range = range(len(self.b.T))\n        # total sample size\n        self.N = len(Tr[0][0][1])\n        self.ϵ = ϵ\n        \n    def ω(self, h=1):\n        ''' Laplace Noise \n        '''\n        return nr.laplace(0, h/self.ϵ)\n    # define normal noise\n    \n    # 2. Run S algorithm\n    def IQR(self, b, db=None):\n        ''' Calc Interquartile Range of partitioned β̂'s\n        '''\n        if db is not None:\n            # Array like O array but with iqr calc for switching old β with new β\n            nb = np.asarray([[np.block([[np.delete(b.T, i, 0)], [db[i][j]]]) for j in range(4)] for i in self.range])\n            IQR = np.asarray([[[np.percentile(k, 75) - np.percentile(k, 25) for k in i.T] for i in j]for j in nb])\n        else:\n            # Original IQR for β\n            IQR = np.asarray([np.percentile(i, 75) - np.percentile(i, 25) for i in b])\n        return IQR\n    \n    def H(self, o=False):\n        ''' compute H for each β̂ and compute H' for alt β̂'s'\n        '''\n        if o:\n            IQR = self.IQR(self.b, self.db)\n            H = np.asarray([[np.log(IQR[i][j]) / np.log(self.base[i]) for j in range(4)]                          \n                           for i in self.range])\n        else:\n            IQR = self.IQR(self.b)\n            H = np.asarray([np.log(IQR) / np.log(self.base[i]) for i in self.range])\n        return H\n    \n    def S(self, BOOL=False):\n        ''' part 2 of the Dwork & Lei (2009) RH Algorithm - compute noise infused SCALE\n        '''\n        H = self.H()\n        dH = self.H(True)\n        # 2.3 compute bins for each H\n        bins = np.asarray([[np.abs(H[i] - dH[i][j] + self.ω()) for j in range(4)] for i in self.range])\n        # 2.4 return TRUE for violation of 2.3 <= 1\n        booL = np.asarray([np.all(i > 1) for i in bins])\n        if BOOL:\n            return booL\n        else:\n            s = np.asarray([self.IQR(self.b) * self.base[i] ** self.ω() for i in self.range])\n            s[booL] = np.inf\n            return s\n\n    # 3. compute h for each s\n    def h(self, s):\n        h = np.asarray([i / math.pow(self.N, 0.25) for i in s])\n        h[h==0] = 1 / math.sqrt(self.N)\n        h[h==np.inf] = 0\n        return h\n    \n    def z(self, h):\n        return np.asarray([\n            self.ω(i) for i in h])\n    \n    def RH(self, s, BOOL=False):\n        ''' part 2 of the Dwork & Lei (2009) RH Algorithm - compute true β + noise\n        '''\n        h = self.h(s)\n        # 3.1 np.abs(alt β̂'s - β̂) <= h array\n        bins = np.asarray([[np.abs(self.b.T[i] - self.db[i][j]) for j in range(4)] for i in self.range])\n        booL = np.asarray([[bins[i][j] > h[i] for j in range(4)] for i in self.range])\n        # return True for violation\n        anybooL = np.asarray([not np.any([not np.any(booL[i][j]) for j in range(4)]) for i in self.range])\n        # 3.2 for TRUE compute β + noise\n        RHb = self.Tb + self.z(h)\n        RHb[anybooL] = None\n        # 3.3 find min(β + noise)\n        if BOOL:\n            return anybooL\n        else:\n            try:\n                np.nanmin(RHb, 0)\n                return np.nanmin(RHb, 0)\n            except:\n                return None\n            \n    def __call__(self, BOOL=False):\n        if BOOL:\n            s = self.S()\n            RH = self.RH(s, BOOL)\n            S = self.S(BOOL)\n            return S, RH\n        else:\n            s = self.S()\n            RH = self.RH(s, BOOL)\n            if RH is None:\n                return \"Too Sensitive\"\n            else:\n                return RH ",
      "metadata": {
        "trusted": true
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "XX, YY, NN, vv = Wrangle(data, aggregate=True)()\nX, Y, N, v = Wrangle(data)()",
      "metadata": {
        "trusted": true
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "OLSA = Methods(XX, YY, NN, vv, \"OLS\")\nOLSP = Methods(X, Y, N, v, \"OLS\")\n\nMMA = Methods(XX, YY, NN, vv, \"MM\")\nMMP = Methods(X, Y, N, v, \"MM\")\n\nSMDMA = Methods(XX, YY, NN, vv, \"SMDM\")\nSMDMP = Methods(X, Y, N, v, \"SMDM\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# 0. calc true statistic (aggregate level)\n# 1. store partition stats (cell/tract level)\nOTr = np.asarray(OLSA(withoutliers=False))\nOr, Oo = OLSP()\nOpsize = OLSP.N\n\nMTr = np.asarray(MMA(withoutliers=False))\nMr, Mo = MMP()\nMpsize = mP.N\n\nSTr = np.asarray(SMDMA(withoutliers=False))\nSr, So = SMDMP()\nSpsize = SMDMP.N",
      "metadata": {
        "trusted": true
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#pickle.dump([OTr, Or, Oo, Opsize, MTr, Mr, Mo, Mpsize, STr, Sr, So, Spsize], open(\"trial.p\", \"wb\"))\n# a1, b1, c1, d1, a2, b2, c2, d2, c3, b3, c3, d3  = pickle.load(open(\"trial.p\",\"rb\"))",
      "metadata": {
        "trusted": true
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "OLSDWORK = DWORK(OTr, Or, Oo, 4, Opsize)\nMMDWORK = DWORK(MTr, Mr, Mo, 4, Mpsize)\nSMDMDWORK = DWORK(STr, Sr, So, 4, Spsize)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "OD = OLSDWORK()\nOB = OLSDWORK(True)\nMD = MMDWORK()\nMB = MMDWORK(True)\nSD = SMDMDWORK()\nSB = SMDMDWORK(True)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 220,
      "outputs": [
        {
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/__main__.py:378: RuntimeWarning: All-NaN slice encountered\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/__main__.py:379: RuntimeWarning: All-NaN slice encountered\n",
          "name": "stderr",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "# Timeit MM v SMDM",
      "metadata": {
        "trusted": true
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "t = OTr[0][0][0]\nprint(abs(t - OD))\nprint(abs(t - MD))\nprint(abs(t - SD))",
      "metadata": {
        "trusted": true
      },
      "execution_count": 221,
      "outputs": [
        {
          "text": "[nan nan]\n[0.00124992 0.02779986]\n[0.00235805 0.00128671]\n",
          "name": "stdout",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "print(np.sum(OB[0]), np.sum(OB[1]))\nprint(np.sum(MB[0]), np.sum(MB[1]))\nprint(np.sum(SB[0]), np.sum(SB[1]))",
      "metadata": {
        "trusted": true
      },
      "execution_count": 231,
      "outputs": [
        {
          "text": "0 111\n0 57\n0 96\n",
          "name": "stdout",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "raw",
      "source": "\n\n\n\n\n\n\n\n",
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": "class Alg_1:\n    '''\n    Chetty and Friedman Algorithm w/ Cell partition\n    '''\n    def __init__(self, data, method, figure=[], ϵ=4.0):\n        ''' Setup selves '''\n        self.D = Wrangle(data)\n        self.col = len(self.D.Key) - 1\n        self.Outlier = np.array([list(i) for i in product([0, 1], repeat = self.col)])\n        self.figure = figure\n        self.method = method\n        self.ϵ = ϵ\n        self.nd75 = ss.norm.ppf(0.75)\n        self.W = np.array([1])\n        self.N = self.D.N\n        self.index = range(self.D.Cells)\n        self.range = range(4) \n        if self.method == \"LmRob\":\n            self.form = ro.Formula(\"y~x\")\n    \n    def ω(self):\n        ''' \n        Draw from Laplace Distribution \n        '''\n        if \"Dwork\" in self.figure:\n            return nr.laplace(0, 1/ϵ)\n        else:\n            return nr.laplace(0, 1/math.sqrt(2))\n    \n    def r(self, x):\n         return range(len(x))\n    \n    def lsR(self, I, O=None):\n        ''' \n        Sort Arrays for Outlier Variations\n        '''\n        if O is None:\n            y = np.asarray(self.D.Y[I]) \n            x = np.asarray(self.D.X[I])\n        else:\n            y = np.block([self.D.Y[I], self.Outlier[O][0]])\n            x = np.block([self.D.X[I], self.Outlier[O][1]])\n\n        return x, y\n\n    def stack(self, x):\n        add = np.ones(len(x))\n        return np.block([[add], [x.T]]).T\n    \n    def lstsq(self, x, y, stderr=False, w=None):\n        '''\n        Calculus Linear Least Squares Optimisation via Numpy\n        '''\n        # Compare to Weight in LSTSQ\n        x = self.stack(x)\n        if w is not None:\n            XX = x.T.dot(np.diag(w)).T\n        else:\n            XX = x\n            \n        betas = np.dot(la.inv(XX.T.dot(x)), XX.T.dot(y))\n        ϵ = y - np.dot(XX, betas)\n        if stderr:\n            if len(y) > 5:\n                stderr = np.sqrt(ϵ.dot(ϵ) / np.sum(np.square(XX.T[1] - np.mean(XX.T[1]))) / (len(ϵ) - self.col))\n            else:\n                stderr = np.inf\n        return betas, ϵ, stderr\n    \n    def MAD(self, ϵ):\n        return np.median(np.absolute(ϵ - np.median(ϵ)))\n    \n    def Tukey(self, u, c=1.547):\n        ρ = np.power(u, 2) / 2 - np.power(u, 4) / (2 * math.pow(c, 2)) + np.power(u, 6) / (6 * math.pow(c, 4))\n        if np.any(np.abs(u) > c):\n            ρ[np.abs(u) > c] = math.pow(c, 2) / 6\n            \n        return ρ\n            \n    def Scale(self, ϵ, w=None, K=0.199):\n        ''' \n        Scale calculation for S-Estimator part of MM\n        '''\n        if w is None:\n            σ = self.MAD(ϵ) / self.nd75 \n        else:\n            σ = np.sqrt(np.sum(np.multiply(np.square(ϵ),w)) / (len(ϵ) * K))\n        \n        return σ\n    \n    def U(self, ϵ, σ):\n        return ϵ / σ\n    \n    def weight(self, u, method=\"S\", it=False):\n        if method==\"S\":\n            if not it:\n            # iteration = 1\n                W = np.square(u / 1.547)\n                if np.any(np.abs(u) > 1.547):\n                    W[np.abs(u) > 1.547] = 0\n                weight = np.square(1 - W)\n            else:\n                weight = self.Tukey(u) / np.square(u)\n        if method==\"MM\":\n            W = np.square(u / 4.685)\n            if np.any(np.abs(u) > 4.685):\n                W[np.abs(u) > 4.685] = 0\n            weight = np.square(1 - W)\n        \n        return weight\n    \n    def OLS(self, I, O=None, stderr=False):\n        # for bug testing\n        R01, R02 = self.lsR(I=I, O=O)\n        R, ϵ, boo = self.lstsq(R01, R02)\n        return R, ϵ, boo\n    \n    def MM(self, I, O=None, stderr=False):\n        ''' \n        MM - Estimator Regression Algorithm as specificied in S 2009\n        '''\n        # 1. Ordinary Least Squares\n        R01, R02 = self.lsR(I=I, O=O)\n        RO1, ϵ, boo = self.lstsq(R01, R02)\n        beta = RO1\n        # 2. S-Estimate Convergence\n        σ = self.Scale(ϵ)\n        U = self.U(ϵ, σ)\n        W = self.weight(U, \"S\", False)\n        S, ϵ, boo = self.lstsq(R01, R02, w=W)\n        # Convergence Loop\n        maxiter = 100\n        for i in range(maxiter):\n            σ = self.Scale(ϵ, w=W)\n            U = self.U(ϵ, σ)\n            W = self.weight(U, \"S\", True)\n            S, ϵ, boo = self.lstsq(R01, R02, w=W)\n            if np.allclose(S, beta, rtol=1e-09):\n                break\n            else:\n                beta = S\n        # use residuals and scale of S-estimate\n        # 3. MM-Estimate Convergence\n        σ = self.Scale(ϵ)\n        for i in range(maxiter):\n            U = self.U(ϵ, σ)\n            W = self.weight(U, \"MM\")\n            M, ϵ, boo = self.lstsq(R01, R02, w=W)\n            if np.allclose(M, beta, rtol=1e-03):\n                break\n            else:\n                beta = M\n\n        if stderr:\n            RW1, RW2, boo = self.lstsq(R01, R02, True, w=W)\n        \n        # return betas, residuals, stderr, scale\n        return M, ϵ, boo, σ\n    \n    def lmrob(self, x, y):\n        '''\n        Run M-Estimator Robust Regression in R via rpy2\n        '''\n        # clear R workspace for looping\n        #base.rm(list='ls()')\n        # Set up formula environment\n        self.form.environment[\"y\"] = ro.r['matrix'](ro.FloatVector(y.flatten()), ncol=1)\n        self.form.environment[\"x\"] = ro.r['matrix'](ro.FloatVector(x.flatten()), ncol=1)\n        # Run Robust Regression\n        lmr = robustbase.lmrob(self.form, method = \"SMDM\", setting=\"KS2014\")\n        # must copy variables because of memory constraint\n        betas = np.array(lmr.rx2(\"coefficients\"), copy=True)\n        scale = np.array(lmr.rx2(\"scale\"), copy=True)\n        residuals = np.array(lmr.rx2(\"residuals\"), copy=True)\n        stderr = np.array(lmr.rx2(\"cov\"), copy=True)[0][0]\n        return betas, residuals, stderr, scale\n    \n    def oR(self, bool=False):\n        '''\n        Specify regression method and run all regression in one/two line(s)\n        '''\n        if self.method == \"OLS\":\n            R0 = np.asarray([self.lsR(i) for i in self.index])\n            self.RA = np.asarray([self.lstsq(i[0], i[1], True) for i in R0])\n            R1 = np.asarray([[self.lsR(i, O=j) for j in self.range] for i in self.index])\n            self.RB = np.asarray([[self.lstsq(i[j][0], i[j][1]) for j in self.range] for i in R1] )           \n        elif self.method == \"MM\":\n            self.RA = np.asarray([self.MM(i, stderr=True) for i in self.index])\n            self.RB = np.asarray([[self.MM(i, j) for j in self.range] for i in self.index])\n        elif self.method == \"LmRob\":\n            R0 = np.asarray([self.lsR(i) for i in self.index])\n            self.RA = np.asarray([self.lmrob(i[0], i[1]) for i in R0])\n            R1 = np.asarray([[self.lsR(i, O=j) for j in self.range] for i in self.index])\n            self.RB = np.asarray([[self.lmrob(i[j][0], i[j][1]) for j in self.range] for i in R1])          \n            \n        if bool:\n            return self.RA.T[0], self.RB.T[0]\n        \n    def LSR(self):\n        self.oR()\n        ''' \n        Calculate Local Sensitivity \n        '''\n        if \"F1\" or \"F2\" in self.figure:\n            LS = np.asarray([[max([abs(self.RB[i][j][0][1] - self.RA[i][0][1]) for j in self.range]), \n                  np.argmax([abs(self.RB[i][j][0][1] - self.RA[i][0][1]) for j in self.range])]\n                  for i in self.index])\n            OSE = np.multiply(self.N, np.transpose(LS)[0])\n            MOSE = max(OSE)\n            if \"F1\" in self.figure:\n                self.MOSE = np.around(max(LS.T[0] * 0.25), 3)\n                self.k = np.argmax(LS.T[0] * 0.25)\n                self.j = LS.T[1][self.k].astype(int)\n                self.cell = str(int(self.index[self.k]))\n            elif \"F2\" in self.figure:\n                self.fD = np.vstack((np.asarray(self.D.N), LS.T[0]))\n                self.MOSE = MOSE\n                self.LS = LS.T[0]\n                self.k = np.argmax(OSE)\n        else:\n            MOSE = max(np.multiply(self.N, \n                [max([abs(self.RB[i][j][0][1] - self.RA[i][0][1]) for j in self.range]) for i in self.index]))\n        return np.transpose([[self.RA[i][j] for j in [0, 2]] for i in self.index]), MOSE\n    \n    def noise(self, θseθ, χ):\n        ''' \n        Add Noise to Statistics \n        '''\n        NN = np.asarray(self.D.N).astype(float)\n        S = NN * self.ϵ\n        noise = lambda x: x * math.sqrt(2)\n        \n        nθ = [i[1] for i in θseθ[0]] + χ * noise(self.ω()) / S\n        sen = np.square(θseθ[1]) + 2 * np.square(χ / S)\n        senθ = np.sqrt(sen.astype(float))\n        nsenθ = senθ + χ * noise(self.ω()) / S\n        nsenθ = np.asarray([\"Sample Size Too Small\" if i==np.inf else i for i in nsenθ])\n        nN = self.D.N * (1 + noise(self.ω()) / S)\n        return nθ, nsenθ, nN\n    \n    def __call__(self):\n        ''' \n        Release Noise Infused Statistics \n        '''\n        m1, m2 = self.LSR()\n        nθ, nsenθ, nN = self.noise(m1, m2)\n        return nθ, nsenθ, nN\n    \n    def plot(self, I=None, O=None):\n        fig, ax = plt.subplots(figsize = (8,6))\n        if \"F1\" in self.figure:\n            self.LSR()\n            # Relevant Data\n            fD = np.vstack((self.D.X[self.k], self.D.Y[self.k]))\n            fo = np.vstack((self.Outlier[self.j][0], self.Outlier[self.j][1]))\n            fLA = lambda λ: self.RA[self.k][0][1]*λ + self.RA[self.k][0][0]\n            fLB = lambda λ: self.RB[self.k][self.j][0][1]*λ + self.RB[self.k][self.j][0][0]\n            \n            # Figure 1 - Effect of Outlier on Regression + MOSE line\n            ax.scatter(fD[0], fD[1])\n            ax.scatter(fo[0], fo[1])\n            l = np.linspace(0, 1, 5)\n            ax.plot(l, fLA(l), label=\"%s-Estimate in Actual Data\"%self.method)\n            ax.plot(l, fLB(l), linestyle = 'dashed', label=\"%s-Estimate with Outlier\"%self.method)\n            # LS line at 25th pctile\n            p25 = [0.25, 0.25]\n            yp25 = [fLA(p25[0]), fLB(p25[1])]\n            ax.plot(p25, yp25, color='black')\n            \n            ax.set_title(\"FIGURE 1: Calculation of Local Sensitivity $= %s$\"%str(self.MOSE), pad=35)\n            ax.set_ylabel(\"Child's Income Rank for Tract %s\"%self.cell)\n            ax.set_xlabel(\"Parent's Income Rank for Tract %s\"%self.cell)\n            \n            plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=2, mode=\"expand\", borderaxespad=0.)\n            ax.set_xlim(-0.025, 1.025)\n            ax.set_ylim(-0.025, 1.025)\n            \n        if \"F2\" in self.figure:\n            self.LSR()\n            fLA = lambda λ: self.MOSE / λ\n            ax.scatter(self.fD[0], self.fD[1])\n            xmin = min(self.D.N)\n            xmax = max(self.D.N)\n            l = [xmin, xmax]\n            ax.plot(l, fLA(l), label=\"$MOSE = χ\\,/\\,N = %s\\,/\\,N $\"%str(np.around(self.MOSE, 2)), color='black')\n            ax.scatter(self.fD[0][self.k], self.fD[1][self.k], label=\"Tract %d\"%self.k)\n            ax.set_title(\"FIGURE 2: Maximum Observed Sensitivity Envelope for %s\"%self.method, pad=35)\n            ax.set_ylabel(\"Local Sensitivity of $̂β_1$ Estimates\")\n            ax.set_xlabel(\"Number of Individuals in Tract\")\n            \n            ax.set_xscale('log')\n            ax.set_yscale('log')\n            \n            ax.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=2, mode=\"expand\", borderaxespad=0.)\n            ax.set_xlim(xmin *0.9, xmax * 1.1)\n            ax.set_ylim(min(self.LS)*0.9, self.MOSE/(xmin *0.9))\n            \n        if \"F3\" in self.figure:\n            # Figure 3\n            l = np.linspace(0, 1, 5)\n            if O is not None:\n                ax.scatter(self.Outlier[O][0], self.Outlier[O][1])\n                ao, bo = self.lsR(I, O)\n                if self.method == \"OLS\":\n                    co, do, eo = self.lstsq(ao, bo)\n                if self.method == \"MM\":\n                    co, do, eo, fo = self.MM(I, O)\n                if self.method == \"LmRob\":\n                    co, do, eo, fo = self.lmrob(ao, bo)\n                lineo = lambda x: co[1]*x + co[0]\n                ax.plot(l, lineo(l))\n            ax.scatter(self.D.X[I], self.D.Y[I])\n            a, b = self.lsR(I)\n            if self.method == \"OLS\":\n                c, d, e = self.lstsq(a, b)\n            if self.method == \"MM\":\n                c, d, e, f = self.MM(I)\n            if self.method == \"LmRob\":\n                c, d, e, f = self.lmrob(a, b)\n            line = lambda x: c[1]*x + c[0]\n            ax.plot(l, line(l))\n            ax.set_xlim(-0.025, 1.025)\n            ax.set_ylim(-0.025, 1.025)\n\n        ax.grid(which='minor', color='w', alpha=0.3)\n        plt.show()",
      "metadata": {
        "trusted": true
      }
    }
  ]
}