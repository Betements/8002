{
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "varInspector": {
      "window_display": false,
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "library": "var_list.py",
          "delete_cmd_prefix": "del ",
          "delete_cmd_postfix": "",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "library": "var_list.r",
          "delete_cmd_prefix": "rm(",
          "delete_cmd_postfix": ") ",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ]
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat_minor": 2,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Classes File",
      "metadata": {
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Local Import\nimport numpy as np\n# import pandas as pd\nimport math\nimport scipy.stats as ss\nimport numpy.linalg as la\nfrom itertools import product\nimport numpy.random as nr\nimport pickle",
      "metadata": {
        "code_folding": [],
        "ExecuteTime": {
          "start_time": "2019-10-16T22:18:15.521592Z",
          "end_time": "2019-10-16T22:18:15.566245Z"
        },
        "trusted": true
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "source": "# R in Python Interface\nimport rpy2\nimport rpy2.robjects as ro\nfrom rpy2.robjects.packages import importr\nimport rpy2.robjects.numpy2ri as rnp\nrobustbase = importr('robustbase')\nbase = importr('base')\nutils = importr('utils')",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-10-16T22:18:17.977Z",
          "end_time": "2019-10-16T22:18:25.967289Z"
        }
      }
    },
    {
      "cell_type": "raw",
      "source": "#diffpriv = importr('diffpriv')\n# Run this in an R console every new session\n# install.packages('diffpriv')\n# install.packages(\"devtools\")\n# devtools::install_github(\"brubinstein/diffpriv\")",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# OBJECTS",
      "metadata": {
        "trusted": true
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "source": "Glossary:\nUI/UII - transforms data range into unit interval\nnpl - subarrays into lists\nwrangle - cells, y, x1, x2, ...\nmethods - ...\ndwork - s, true + noise\nmose - noiseb, noisese, noiseN, epsilon, mose\nmse - NORM",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "''' Transform Range of Data into Unit Interval Space '''\ndef UI(x, b=True):\n    ''' For Univariate and Aggregate Regression '''\n    if b:\n        mimi = np.min([i for j in x for i in j])\n        mama = np.max([i for j in x for i in j])\n        return (x - mimi) / (mama - mimi)\n    else:\n        return (x - np.min(x)) / (np.max(x) - np.min(x))\ndef UII(x, b=False):\n    ''' For Multivariate Regression '''\n    if b:\n        mimi = np.asarray([np.min(i) for i in x.T])\n        mama = np.asarray([np.max(i) for i in x.T])\n    else:\n        mm = np.asarray([i for j in x for i in j]).T\n        mimi = np.asarray([np.min(i) for i in mm])\n        mama = np.asarray([np.max(i) for i in mm])\n    \n    return [(i - mimi) / (mama - mimi) for i in x]\n\n''' Converts subarrays into lists '''\ndef npl(x):\n    return np.asarray([i.tolist() for i in x])",
      "metadata": {
        "trusted": true
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "source": "WRANGLE",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class Wrangle:\n    ''' Data Wrangling Class for Test Data \n    '''\n    def __init__(self, data, aggregate=False):\n        ''' Numpy-nize Census Tract \n        '''\n        if 'PUMA5' in data:\n            data = data.drop(['PUMA5'], axis=1)\n        self.Key = data.keys()\n        if aggregate:\n            Y = np.asarray(data.loc[:, self.Key[1]])\n            if len(self.Key) > 3:\n                X = np.asarray(data.loc[:,self.Key[2]:])\n            else:\n                X = np.asarray(data.loc[:, self.Key[2]])\n            self.N = len(X)\n        else:\n            if len(self.Key) > 3:\n                Cell = np.unique(np.array(data[self.Key[0]])).astype(int)\n                Y = [np.asarray(data.loc[data.loc[:, self.Key[0]]== i, self.Key[1]]) for i in Cell]\n                X = [np.asarray(data[data.Cell==i].loc[:,self.Key[2]:]) for i in Cell]\n                self.N = np.asarray([len(i) for i in X])\n            else:\n                Cell = np.unique(np.array(data[self.Key[0]])).astype(int)\n                Y = [np.asarray(data.loc[data.loc[:, self.Key[0]]== i, self.Key[1]]) for i in Cell]\n                X = [np.asarray(data.loc[data.loc[:, self.Key[0]]== i, self.Key[2]]) for i in Cell]\n                self.N = np.asarray([len(i) for i in X])\n        self.X = X\n        self.Y = Y \n        self.v = len(self.Key) - 1\n                \n    def __call__(self):\n        return self.X, self.Y, self.N, self.v",
      "metadata": {
        "trusted": true
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "source": "np.asarray([list(i) for i in product([0, 1], repeat = 2)])\nnp.asarray([list(i) for i in product([-1, 2], repeat = 2)])",
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": "Methods",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class Methods:\n    ''' Class of regression methods and their parts\n    '''\n    def __init__(self, X, Y, N, v, method=[]):\n        ''' Make sure you are using at most 2-dimensional arrays for X, Y\n        '''\n        # We assume X is organised by index with dependent variables inside each\n        self.X = np.asarray(X)\n        self.Y = np.asarray(Y)\n        self.method = method\n        if \"SMDM\" in self.method:\n            self.form = ro.Formula(\"y~x\")\n        self.N = N\n        self.v = v\n#         self.o = np.asarray([list(i) for i in product([0, 1], repeat = self.v)])\n        self.o = np.asarray([list(i) for i in product([0, 1], repeat = self.v)])\n        if type(self.N) is int:\n            self.index = [None]\n        else:\n            self.index = range(len(self.N))\n        self.range = range(len(self.o))\n        self.nd75 = ss.norm.ppf(0.75)\n        if \"Winsorize\" in self.method:\n            self.win = True\n        else:\n            self.win = False\n        if \"SE\" in self.method:\n            self.SE = True\n        else:\n            self.SE = False\n            \n        # Maxiter-rer\n        for i in self.method:\n            try:\n                maxiter = int(i)\n            except:\n                pass\n        try:\n            self.maxiter = maxiter\n        except:\n            self.maxiter = 100\n    \n    def sort(self, I=None, O=None, win=False):\n        ''' Sort arrays and append outlier variations\n        '''\n        if I is None:\n            y = self.Y\n            x = self.X\n        else:\n            y = self.Y[I]\n            x = self.X[I]\n        \n        if O is not None:\n            y = np.block([y, self.o[O][0]])\n            if self.v > 2 :\n                x = np.vstack([x, self.o[O][1:]])\n            else:\n                x = np.block([x, self.o[O][1]])\n        \n        if win:\n            x = ss.mstats.winsorize(x, limits=0.05)\n            x = x.filled()\n            y = ss.mstats.winsorize(y, limits=0.05)\n            y = y.filled()\n        \n        return x, y\n    \n    def stack(self, x):\n        ''' Add constant to estimate \n        '''\n        return np.block([[np.ones(len(x))], [x.T]]).T\n    \n    def HCE(self, X, r, a):\n        ''' Calculate Heteroscedasticity-consistent standard errors\n        '''\n        HCE = np.dot(np.dot(a, X.T.dot(np.dot(np.diag(np.square(r)), X))), a)\n        return np.array(np.diag(HCE))\n    \n    def lstsq(self, x, y, w=None, se=False):\n        ''' Calculus linear least-squares\n        '''\n        x = self.stack(x)\n        if w is not None:\n            X = x.T.dot(np.diag(w)).T\n        else:\n            X = x\n        a = la.inv(X.T.dot(x))\n        β̂ = np.dot(a, X.T.dot(y))\n        r = y - np.dot(X, β̂)\n        if se:\n            se = self.HCE(X, r, a)\n            #se = np.sqrt(r.dot(r) / np.sum(np.square(X.T[1] - np.mean(X.T[1]))) / (len(r) - self.v))\n        return β̂, r, se\n\n    # OLS Method:\n    def OLS(self, I=None, O=None, se=False):\n        x, y = self.sort(I=I, O=O, win=self.win)\n        β̂, r, se = self.lstsq(x, y, None, se)\n        return β̂, r, se\n\n    # Most B-Robust Method:\n    def Huber(self, I=None, O=None, se=False):\n        x, y = self.sort(I=I, O=O, win=self.win)\n        norm = la.norm(x)\n        x = x / norm\n        y = y / norm\n        β̂, r, se = self.lstsq(x, y, None, se)\n        return β̂, r, se\n        \n    # MM Method:\n    def MAD(self, r):\n        ''' Median Absolute Deviation \n        '''\n        return np.median(np.absolute(r - np.median(r)))\n    \n    def Tukey(self, u, c=1.547):\n        ''' Tukey's biweight function ''' \n        ρ = np.power(u, 2) / 2 - np.power(u, 4) / (2 * math.pow(c, 2)) + np.power(u, 6) / (6 * math.pow(c, 4))\n        if np.any(np.abs(u) > c):\n            ρ[np.abs(u) > c] = math.pow(c, 2) / 6            \n        return ρ\n            \n    def σ(self, r, w=None, K=0.199):\n        ''' The scale for the S-Estimator we wish to minimize\n        '''\n        if w is None:\n            σ = self.MAD(r) / self.nd75 \n        else:\n            σ = np.sqrt(np.sum(np.multiply(np.square(r),w)) / (len(r) * K))\n        return σ\n    \n    def u(self, r, σ):\n        ''' Bisquare ratio \n        '''\n        return r / σ\n    \n    def weight(self, u, method=\"S\", it=False):\n        ''' Estimator Reweighing\n        '''\n        if method==\"S\":\n            if not it:\n            # iteration = 1\n                w = np.square(u / 1.547)\n                if np.any(np.abs(u) > 1.547):\n                    w[np.abs(u) > 1.547] = 0\n                weight = np.square(1 - w)\n            else:\n                weight = self.Tukey(u) / np.square(u)\n        if method==\"MM\":\n            w = np.square(u / 4.685)\n            if np.any(np.abs(u) > 4.685):\n                w[np.abs(u) > 4.685] = 0\n            weight = np.square(1 - w)\n        return weight\n\n    def MM(self, I=None, O=None, SE=True):\n        ''' MM-Estimator regression algorithm as specified in Susanti 2009\n        '''\n        x, y = self.sort(I, O, win=self.win)\n        β̂, r, se = self.lstsq(x, y, None, False)\n        # 2. S-Estimation\n        # 1st iteration\n        σ = self.σ(r)\n        u = self.u(r, σ)\n        w = self.weight(u, \"S\", False)\n        β̂, r, se = self.lstsq(x, y, w, False)\n        # IRWLS\n        for i in range(self.maxiter):\n            σ = self.σ(r, w)\n            u = self.u(r, σ)\n            w = self.weight(u, \"S\", True)\n            S, r, se = self.lstsq(x, y, w, False)\n            if np.allclose(S, β̂, rtol=1e-09):\n#             if math.isclose(la.norm(S - β̂), 0, abs_tol=1e-09):\n                break\n            else:\n                β̂ = S\n        # We use the MAD of the residuals of the S-estimator\n#         σ = self.σ(r)\n        # Slightly Worse accuracy but closer to OLS estimate\n        σ = self.σ(r, w)\n        # 3. MM-Estimate Convergence \n        for i in range(self.maxiter):\n            u = self.u(r, σ)\n            w = self.weight(u, \"MM\")\n            M, r, se = self.lstsq(x, y, w, False)\n#             if math.isclose(la.norm(M - β̂), 0, abs_tol=1e-03):\n            if np.allclose(M, β̂, rtol=1e-04):\n                break\n            else:\n                β̂ = M\n        if SE:\n            β̂s, rs, se = self.lstsq(x, y, w, SE)\n        return M, r, se, σ   \n        \n    # SMDM Method:\n    def SMDM(self, I=None, O=None):\n        ''' Run SMDM from R's 'lmrob' package via rpy2\n        '''\n        x, y = self.sort(I, O, win=self.win)\n        try:\n            nr, nc = x.shape\n        except:\n            nr = x.shape[0]\n            nc = 1\n        # Set up formula environment\n        rnp.activate()\n        self.form.environment[\"y\"] = ro.r['matrix'](y, nrow=nr, ncol=1)\n        self.form.environment[\"x\"] = ro.r['matrix'](x, nrow=nr, ncol=nc)\n        # Run Robust Regression\n        lmr = robustbase.lmrob(self.form, method = \"SMDM\", setting=\"KS2014\")\n        # must copy variables because of memory constraint\n        betas = np.array(lmr.rx2(\"coefficients\"), copy=True)\n        scale = np.array(lmr.rx2(\"scale\"), copy=True)\n        residuals = np.array(lmr.rx2(\"residuals\"), copy=True)\n        stderr = np.array(np.diag(lmr.rx2(\"cov\")), copy=True)\n        return betas, residuals, stderr, scale\n    \n    def __call__(self, withoutliers=True):\n        ''' Specify regression method and run all regression in one/two line(s)\n        '''\n        if \"OLS\" in self.method:\n            if self.index is None:\n                regression = self.OLS(None, None, self.SE)\n                if withoutliers:\n                    withoutliers = np.asarray([self.OLS(None, O, self.SE) for O in self.range])\n            else:\n                regression = np.asarray([self.OLS(I, None, self.SE) for I in self.index])\n                if withoutliers:\n                    withoutliers = np.asarray([[self.OLS(I, O, self.SE) for O in self.range] for I in self.index]) \n        if \"H\" in self.method:\n            if self.index is None:\n                regression = self.Huber(None, None, self.SE)\n                if withoutliers:\n                    withoutliers = np.asarray([self.Huber(None, O, self.SE) for O in self.range])\n            else:\n                regression = np.asarray([self.Huber(I, None, self.SE) for I in self.index])\n                if withoutliers:\n                    withoutliers = np.asarray([[self.Huber(I, O, self.SE) for O in self.range] for I in self.index])    \n        elif \"MM\" in self.method:\n            if self.index is None:\n                regression = self.MM(None, None, self.SE)\n                if withoutliers:\n                    withoutliers = np.asarray([self.MM(None, O, self.SE) for O in self.range])\n            else:\n                regression = np.asarray([self.MM(I, None, self.SE) for I in self.index])\n                if withoutliers:\n                    withoutliers = np.asarray([[self.MM(I, O, self.SE) for O in self.range] for I in self.index])\n        elif \"SMDM\" in self.method:\n            if self.index is None:\n                regression = self.SMDM(None, None)\n                if withoutliers:\n                    withoutliers = np.asarray([self.SMDM(None, O) for O in self.range])\n            else:\n                regression = np.asarray([self.SMDM(I, None) for I in self.index])\n                if withoutliers:\n                    withoutliers = np.asarray([[self.MM(I, O) for O in self.range] for I in self.index])\n        return regression, withoutliers",
      "metadata": {
        "trusted": true
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "source": "DWORK\n''' We have assumed aggregate data instead of tract partitioning\n            Since we have access to only a 5% sample of any Census\n            ### b is org by cells with b's in cols\n            ### B.T is B0,B1 arrays\n            ### db is org by cell with alt's within - 3dim\n            ### dB.T splits B_0 and B_1 alts into two arrays with 4 subarrys each '''",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class DWORK:\n    def __init__(self, Tr, r, o, ϵ, psize, method=[\"Laplace\", \"BETA\", \"IQR\"]):\n        ''' Dwork & Lei Release Algorithm\n            Developed for releasing true β w/ noise\n            Can be used for releasing SE's as well\n        '''\n        self.method = method\n        # length of each cell\n        self.n = psize\n        # base for each cell (for S algorithm)\n        self.base = np.asarray([1 + 1 / np.log(i) for i in self.n])\n        \n        # store β̂'s and β̂ᵢ's:\n        if \"BETA\" in self.method:\n            self.Tb = Tr[0][0]\n            self.b = npl(r.T[0]).T\n            self.db = npl(o.T[0].T)\n        elif \"SE\" in self.method:\n            self.Tb = Tr[0][2]\n            self.b = npl(r.T[2]).T\n            self.db = npl(o.T[2].T)\n        if \"MAD\" in self.method:\n            self.residuals = npl(r.T[1]).T\n            self.dresiduals = npl(o.T[1].T)\n            self.nd75 = ss.norm.ppf(0.75)\n\n        # number of partitions & range\n        self.pee = np.sum(self.n) / self.n\n        self.p = len(self.n)\n        self.range = range(self.p)\n        # total sample size\n        self.N = np.sum(self.n)\n        self.ϵ = ϵ\n        # define delta parameter\n        ndivp = self.N / self.p\n        self.δ = 0.5 * math.pow(ndivp, (- ϵ) * math.log(ndivp))\n        self.RANGE = range(len(self.db[0]))\n            \n        if \"MAD\" in self.method:\n            self.VRANGE = []\n            self.vars = len(self.b.T[0])\n        else:\n            self.VRANGE = range(len(self.b.T[0]))\n    \n    def ω(self, h=1, method=[\"Laplace\"]):\n        ''' Laplace of Gaussian Mechanism\n        '''\n        if \"Laplace\" in method:\n            return nr.laplace(0, h/self.ϵ)\n        elif \"Gaussian\" in method:\n            return nr.normal(0, h * math.sqrt(2 * math.log(1.25 / self.δ)) / self.ϵ)\n        else:\n            raise ValueError('No Mechanism Specified')\n    \n    # 2. Run S algorithm\n    def IQR(self, b, db=None):\n        ''' Function: Interquartile Range of partitioned β̂'s\n        '''\n        if db is not None:\n            # Array like O array but with iqr calc for switching old β with new β\n            nb = np.asarray([[np.block([[np.delete(b.T, i, 0)], [db[i][j]]]) for j in self.RANGE] for i in self.range])\n            IQR = np.asarray([[[np.percentile(k, 75) - np.percentile(k, 25) for k in i.T] for i in j]for j in nb])\n        else:\n            # Original IQR for β\n            IQR = np.asarray([np.percentile(i, 75) - np.percentile(i, 25) for i in b])\n        return IQR\n    \n    def mad(self, r):\n        ''' Median Absolute Deviation \n        '''\n        return np.median(np.absolute(r - np.median(r))) / self.nd75\n\n    def BMAD(self, b, db=None):\n        ''' NEW: MAD for Beta's instead of residuals\n        '''\n        if db is not None:\n            # Array like O array but with iqr calc for switching old β with new β\n            nb = np.asarray([[np.block([[np.delete(b.T, i, 0)], [db[i][j]]]) for j in self.RANGE] for i in self.range])\n            BMAD = np.asarray([[[mad(k) for k in i.T] for i in j]for j in nb])\n        else:\n            # Original IQR for β\n            BMAD = np.asarray([mad(i) for i in b])\n        return BMAD\n    \n    def MAD(self, r, dr=None):\n        ''' NEW: We calculate the MAD for the residuals of each β̂; instead of the IQR\n        '''\n        if dr is not None:\n            # Array like O array but with iqr calc for switching old β with new β\n            nb = []\n            for i in self.range:\n                ab = []\n                for j in self.RANGE:\n                    K = r.tolist()\n                    K[i] = dr[i][j]\n                    ab.append(K)\n                nb.append(ab)\n            nb = np.asarray(nb)\n            # Returns Median Absolute Deviation for each altered partition\n            MAD = np.asarray([[[self.mad(k) for k in i.T] for i in j]for j in nb])\n            # Returns IQR of MAD - n.b.: you have instead chosen to go for the norm\n#             IQR = np.asarray([[np.percentile(j, 75) - np.percentile(j, 25) for j in i] for i in MAD])\n            GMAD = np.asarray([[la.norm(j) for j in i] for i in MAD])\n            \n        else:\n            # Original MAD for β\n            # Returns Median Absolute Deviation for each partition\n            MAD = np.asarray([self.mad(i) for i in r])\n#             IQR = np.asarray(np.percentile(MAD, 75) - np.percentile(MAD, 25))\n            GMAD = np.asarray(la.norm(MAD))\n        return GMAD\n        \n    \n    def H(self, o=False):\n        ''' Part 1: H\n            - Compute H for each β̂ and compute H' for alt β̂'s'\n        '''\n        if o:\n            if \"MAD\" in self.method:\n                SCALE = self.MAD(self.residuals, self.dresiduals)\n                H = np.asarray([np.log(SCALE[i]) / np.log(self.base[i]) for i in self.range])\n            else:\n                if \"IQR\" in self.method:\n                    SCALE = self.IQR(self.b, self.db)\n                elif \"BMAD\" in self.method:\n                    SCALE = self.BMAD(self.b, self.db)\n                H = np.asarray([[np.log(SCALE[i][j]) / np.log(self.base[i]) for j in self.RANGE]for i in self.range])                \n        else:\n            if \"MAD\" in self.method:\n                SCALE = self.MAD(self.residuals)\n            else:\n                if \"IQR\" in self.method:\n                    SCALE = self.IQR(self.b)  \n                elif \"BMAD\" in self.method:\n                    SCALE = self.BMAD(self.b)  \n            H = np.asarray([np.log(SCALE) / np.log(self.base[i]) for i in self.range])\n        return H\n    \n    def S(self, BOOL=False):\n        ''' Part 2: S Algorithm\n            - Compute Noise-infused SCALE for β̂'s\n        '''\n        # Log IQR / Log 1 + 1/n\n        H = self.H()\n        dH = self.H(True)\n        \n        # 2.3 - Bins for each H\n        bins = np.asarray([[np.abs(H[i] - dH[i][j]) for j in self.RANGE] for i in self.range])\n        \n        # 2.4 return TRUE for violation of 2.3 <= 1\n        if \"MAD\" in self.method:\n            booL = np.asarray([np.any(bins[i] > 1) for i in self.range])\n            s = np.asarray([self.MAD(self.residuals) * (self.base[i] ** abs(self.ω())) for i in self.range])\n            s[booL] = np.nan            \n        else:\n            booL = np.asarray([[np.any(bins[i].T[j] > 1) for j in self.VRANGE] for i in self.range])\n            if \"IQR\" in self.method:\n                s = np.asarray([self.IQR(self.b) * (self.base[i] ** abs(self.ω())) for i in self.range])\n            elif \"BMAD\" in self.method:\n                s = np.asarray([self.BMAD(self.b) * (self.base[i] ** abs(self.ω())) for i in self.range])\n            s[booL] = np.nan\n\n        if BOOL:\n            s = booL\n        return s\n\n    # 3. compute h for each s\n    def h(self, s):\n        h = np.asarray([s[i] / math.pow(self.N, 1 / (2 * self.pee[i])) for i in self.range])\n        return h\n    \n    def z(self, h):\n        if \"MAD\" in self.method:\n            stackh = np.ones((self.vars, self.p)) * h\n            return np.asarray([[self.ω(j, self.method) for j in i] for i in stackh.T])\n        else:\n            return np.asarray([[self.ω(j, self.method) for j in i] for i in h])        \n    \n    def RH(self, s, BOOL=False):\n        ''' Part 3: Release\n            - Compute True β + Noise\n        '''\n        h = self.h(s)\n        # 3.1 np.abs(alt β̂'s - β̂) <= h array\n        bins = np.asarray([[np.abs(self.b.T[i] - self.db[i][j]) for j in self.RANGE] for i in self.range])\n        \n        # return True for violation        \n        if \"MAD\" in self.method:\n            anybooL = np.asarray([np.any(bins[i] > h[i]) for i in self.range])\n            boolie = [np.any(i) for i in anybooL]\n        else:\n            anybooL = np.asarray([[np.any(bins[i].T[j] > h[i][j]) for j in self.VRANGE] for i in self.range])\n            boolie = [np.any(i) for i in anybooL]            \n            \n        # 3.2 for TRUE compute β + noise\n        RHb = self.Tb + self.z(h)\n        RHb[anybooL] = np.nan   \n        if BOOL:\n            return anybooL\n        elif \"Local\" in self.method:\n            return RHb\n        else:\n            # 3.3 find min(β + noise)\n            similar = [la.norm(self.Tb - i) for i in RHb]\n            try:\n                index = np.nanargmin(similar)\n                return RHb[index]\n            except:\n                return np.full(len(self.b.T[0]), np.nan)\n            \n    def __call__(self, BOOL=False):\n        ''' Set up so one can return Boolean Arrays for both S and RH algorithm \n        ''' \n        s = self.S()\n        RH = self.RH(s, BOOL)\n        if BOOL:\n            S = self.S(True)\n        elif \"Local\" in self.method:\n            if \"MAD\" in self.method:\n                S = self.MAD(self.residuals) * ((1 + 1 / np.log(self.N)) ** self.ω())\n            else:\n                if \"IQR\" in self.method:\n                    S = self.IQR(self.b) * ((1 + 1 / np.log(self.N)) ** self.ω())\n                elif \"BMAD\" in self.method:\n                    S = self.BMAD(self.b) * ((1 + 1 / np.log(self.N)) ** self.ω())\n        else:\n            S = s\n            \n        return S, RH",
      "metadata": {
        "trusted": true
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "source": "Mose",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class MOSE:\n    ''' Release Algorithm by Chetty & Friedman\n    '''\n    def __init__(self, regression, withoutliers, ϵ, N, method=[\"Laplace\"]):\n        ''' We assume we are working at the PUMS Code Level, not the Tract Level\n        '''\n        self.RA = regression\n        self.RB = withoutliers\n        self.range = range(4)\n        if type(N) is int:\n            self.N = 1\n            self.L = self.N\n            self.index = range(self.N)\n        else:\n            self.N = N\n            self.L = len(self.N)\n            self.index = range(self.L)\n        self.method = method\n        self.ϵ = ϵ\n            \n    def ω(self, size):\n        ''' Draws from Laplace or Normal Distribution \n        '''\n        if \"Laplace\" in self.method:\n            return nr.laplace(0, 1/math.sqrt(2), size)\n        elif \"Gaussian\" in self.method:\n            return nr.normal(0, 1, size)\n    \n    def LS(self):\n        ''' Calculate Local Sensitivity \n        '''\n        return np.asarray([np.max(np.abs(npl(self.RB.T[0].T)[i].T[1] - npl(self.RA.T[0].T)[i][1])) for i in self.index])\n    \n    def mOSE(self):\n        ''' Calculate Maximum Observed Local Sensitivity '''\n        MOSE = np.max(self.N * self.LS())\n        return np.asarray([npl(self.RA.T[i]).T for i in [0, 2]]), MOSE\n    \n    \n    def noise(self, θseθ, χ):\n        ''' Add Noise to Statistics \n        '''\n        S = np.asarray(self.N).astype(float)\n        noise = lambda x: x * math.sqrt(2) * self.ϵ\n        \n        nθ = np.asarray([i + χ * noise(self.ω(self.L)) / S for i in θseθ[0]])\n        senθ = np.asarray(np.sqrt([np.square(i) + 2 * np.square(χ / S) for i in θseθ[1]])) \n        nsenθ = np.asarray([i + χ * noise(self.ω(self.L)) / S for i in senθ])\n        nN = self.N * (1 + noise(self.ω(self.L)) / S)\n        return nθ, nsenθ, nN\n    \n    def Ω(self, χ, N):\n        ''' Draws from Laplace Distribution \n        '''\n        scale = χ / (N * self.ϵ)\n        return nr.laplace(0, scale, self.L)\n\n    def correctnoise(self, θseθ, χ):\n        ''' Add Noise to Statistics \n        '''\n        S = np.asarray(self.N).astype(float)\n        \n        nθ = np.asarray([i + self.Ω(χ, S) for i in θseθ[0]])\n        senθ = np.asarray(np.sqrt([np.square(i) + 2 * np.square(χ / S) for i in θseθ[1]])) \n        nsenθ = np.asarray([i + self.Ω(χ, S) for i in senθ])\n        nN = self.N * (1 + self.Ω(χ, S) / χ)\n        return nθ, nsenθ, nN\n#         senθ = np.square(θseθ[1][1]) + 2 * np.square(χ / S) \n#         np.sqrt(sen.astype(float))\n#         senθ + χ * noise(self.ω(self.L)) / S\n#         nsenθ = np.asarray([\"Sample Size Too Small\" if i==np.inf else i for i in nsenθ])\n\n    def __call__(self, LS=False):\n        ''' Release Noise Infused Statistics \n        '''\n        if LS:\n            return self.LS()\n        else:\n            θandSEθ, mose = self.mOSE()\n            if \"Correct\" in self.method:\n                nθ, nsenθ, nN = self.correctnoise(θandSEθ, mose)\n            else:\n                nθ, nsenθ, nN = self.noise(θandSEθ, mose)\n            return nθ.T, nsenθ.T, nN, self.ϵ, mose",
      "metadata": {
        "trusted": true
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "source": "MSE",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class MSE:\n    ''' We compute MSE's for the DWORK algorithm outputs for different epsilons\n    '''\n    def __init__(self, Tr, r, o, psize, method=[\"Laplace\", \"DWORK\", \"BETA\"]):    \n        self.T = Tr[0][0]\n        self.Tr = Tr\n#         self.To = To\n        self.r = r\n        self.o = o\n        self.psize = psize\n        self.method = method\n        self.ϵ = [j/10 for j in range(1, 11)]\n        if \"MAD\" in self.method:\n            self.k = 2\n        else:\n            self.k = 10\n        \n    def __call__(self):\n        MSE = []\n        for ϵ in self.ϵ:\n            diff = []\n            # We repeat 10 times\n            for j in range(self.k):\n                if \"DWORK\" in self.method:\n                    s, noise = DWORK(self.Tr, self.r, self.o, ϵ, self.psize, self.method)()\n                elif \"MOSE\" in self.method:\n                    noise, nsenθ, nN, ϵ, mose = MOSE(self.r, self.o, ϵ, self.psize, self.method)()\n#                     index = np.nanargmin([la.norm(self.T - i) for i in nθ])\n                try:\n                    # Squared Error for Beta Vector\n                    l2norm = np.nanmin([np.square(la.norm(self.T - i)) for i in noise])\n                except:\n                    l2norm = np.nan\n#                 diff.append(np.square(np.subtract(noise, self.T)))\n                diff.append(l2norm)\n            # Compute Mean of Squared Error\n            MSE.append(np.nanmean(diff))\n        # Return MSE Array\n        return MSE",
      "metadata": {
        "trusted": true
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "source": "#Tests",
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": "%cd /home/nbuser/library/DP/Data/DATA\nwith open(\"99C.pickle\", \"rb\") as handle:\n    x, y, n, v = pickle.load(handle)\nwith open(\"IAC.pickle\", \"rb\") as handle:\n    X, Y, N, V = pickle.load(handle)\n\n%cd /home/nbuser/library/DP/Data/ESTIMATES\nwith open(\"99T.pickle\", \"rb\") as handle:\n    tr, to = pickle.load(handle)\nwith open(\"IAT.pickle\", \"rb\") as handle:\n    TR, TO = pickle.load(handle)\nwith open(\"99S.pickle\", \"rb\") as handle:\n    r, o = pickle.load(handle)\nwith open(\"IAS.pickle\", \"rb\") as handle:\n    R, O = pickle.load(handle)",
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": "dwr = DWORK(tr, r, o, 0.5, n, [\"Laplace\", \"BETA\", \"MAD\"])\nDWR = DWORK(tr, r, o, 0.5, n, [\"Laplace\", \"BETA\", \"IQR\"])",
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": "sss, rrr = dwr()\nSSS, RRR = DWR()",
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": "RRR",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "source": "Old Stuff Below\n\n\n",
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": "class Alg_1:\n    '''\n    Chetty and Friedman Algorithm w/ Cell partition\n    '''\n    def __init__(self, data, method, figure=[], ϵ=4.0):\n        ''' Setup selves '''\n        self.D = Wrangle(data)\n        self.col = len(self.D.Key) - 1\n        self.Outlier = np.array([list(i) for i in product([0, 1], repeat = self.col)])\n        self.figure = figure\n        self.method = method\n        self.ϵ = ϵ\n        self.nd75 = ss.norm.ppf(0.75)\n        self.W = np.array([1])\n        self.N = self.D.N\n        self.index = range(self.D.Cells)\n        self.range = range(4) \n        if self.method == \"LmRob\":\n            self.form = ro.Formula(\"y~x\")\n    \n    def ω(self):\n        ''' \n        Draw from Laplace Distribution \n        '''\n        if \"Dwork\" in self.figure:\n            return nr.laplace(0, 1/ϵ)\n        else:\n            return nr.laplace(0, 1/math.sqrt(2))\n    \n    def r(self, x):\n         return range(len(x))\n    \n    def lsR(self, I, O=None):\n        ''' \n        Sort Arrays for Outlier Variations\n        '''\n        if O is None:\n            y = np.asarray(self.D.Y[I]) \n            x = np.asarray(self.D.X[I])\n        else:\n            y = np.block([self.D.Y[I], self.Outlier[O][0]])\n            x = np.block([self.D.X[I], self.Outlier[O][1]])\n\n        return x, y\n\n    def stack(self, x):\n        add = np.ones(len(x))\n        return np.block([[add], [x.T]]).T\n    \n    def lstsq(self, x, y, stderr=False, w=None):\n        '''\n        Calculus Linear Least Squares Optimisation via Numpy\n        '''\n        # Compare to Weight in LSTSQ\n        x = self.stack(x)\n        if w is not None:\n            XX = x.T.dot(np.diag(w)).T\n        else:\n            XX = x\n            \n        betas = np.dot(la.inv(XX.T.dot(x)), XX.T.dot(y))\n        ϵ = y - np.dot(XX, betas)\n        if stderr:\n            if len(y) > 5:\n                stderr = np.sqrt(ϵ.dot(ϵ) / np.sum(np.square(XX.T[1] - np.mean(XX.T[1]))) / (len(ϵ) - self.col))\n            else:\n                stderr = np.inf\n        return betas, ϵ, stderr\n    \n    def MAD(self, ϵ):\n        return np.median(np.absolute(ϵ - np.median(ϵ)))\n    \n    def Tukey(self, u, c=1.547):\n        ρ = np.power(u, 2) / 2 - np.power(u, 4) / (2 * math.pow(c, 2)) + np.power(u, 6) / (6 * math.pow(c, 4))\n        if np.any(np.abs(u) > c):\n            ρ[np.abs(u) > c] = math.pow(c, 2) / 6\n            \n        return ρ\n            \n    def Scale(self, ϵ, w=None, K=0.199):\n        ''' \n        Scale calculation for S-Estimator part of MM\n        '''\n        if w is None:\n            σ = self.MAD(ϵ) / self.nd75 \n        else:\n            σ = np.sqrt(np.sum(np.multiply(np.square(ϵ),w)) / (len(ϵ) * K))\n        \n        return σ\n    \n    def U(self, ϵ, σ):\n        return ϵ / σ\n    \n    def weight(self, u, method=\"S\", it=False):\n        if method==\"S\":\n            if not it:\n            # iteration = 1\n                W = np.square(u / 1.547)\n                if np.any(np.abs(u) > 1.547):\n                    W[np.abs(u) > 1.547] = 0\n                weight = np.square(1 - W)\n            else:\n                weight = self.Tukey(u) / np.square(u)\n        if method==\"MM\":\n            W = np.square(u / 4.685)\n            if np.any(np.abs(u) > 4.685):\n                W[np.abs(u) > 4.685] = 0\n            weight = np.square(1 - W)\n        \n        return weight\n    \n    def OLS(self, I, O=None, stderr=False):\n        # for bug testing\n        R01, R02 = self.lsR(I=I, O=O)\n        R, ϵ, boo = self.lstsq(R01, R02)\n        return R, ϵ, boo\n    \n    def MM(self, I, O=None, stderr=False):\n        ''' \n        MM - Estimator Regression Algorithm as specificied in S 2009\n        '''\n        # 1. Ordinary Least Squares\n        R01, R02 = self.lsR(I=I, O=O)\n        RO1, ϵ, boo = self.lstsq(R01, R02)\n        beta = RO1\n        # 2. S-Estimate Convergence\n        σ = self.Scale(ϵ)\n        U = self.U(ϵ, σ)\n        W = self.weight(U, \"S\", False)\n        S, ϵ, boo = self.lstsq(R01, R02, w=W)\n        # Convergence Loop\n        maxiter = 100\n        for i in range(maxiter):\n            σ = self.Scale(ϵ, w=W)\n            U = self.U(ϵ, σ)\n            W = self.weight(U, \"S\", True)\n            S, ϵ, boo = self.lstsq(R01, R02, w=W)\n            if np.allclose(S, beta, rtol=1e-09):\n                break\n            else:\n                beta = S\n        # use residuals and scale of S-estimate\n        # 3. MM-Estimate Convergence\n        σ = self.Scale(ϵ)\n        for i in range(maxiter):\n            U = self.U(ϵ, σ)\n            W = self.weight(U, \"MM\")\n            M, ϵ, boo = self.lstsq(R01, R02, w=W)\n            if np.allclose(M, beta, rtol=1e-03):\n                break\n            else:\n                beta = M\n\n        if stderr:\n            RW1, RW2, boo = self.lstsq(R01, R02, True, w=W)\n        \n        # return betas, residuals, stderr, scale\n        return M, ϵ, boo, σ\n    \n    def lmrob(self, x, y):\n        '''\n        Run M-Estimator Robust Regression in R via rpy2\n        '''\n        # clear R workspace for looping\n        #base.rm(list='ls()')\n        # Set up formula environment\n        self.form.environment[\"y\"] = ro.r['matrix'](ro.FloatVector(y.flatten()), ncol=1)\n        self.form.environment[\"x\"] = ro.r['matrix'](ro.FloatVector(x.flatten()), ncol=1)\n        # Run Robust Regression\n        lmr = robustbase.lmrob(self.form, method = \"SMDM\", setting=\"KS2014\")\n        # must copy variables because of memory constraint\n        betas = np.array(lmr.rx2(\"coefficients\"), copy=True)\n        scale = np.array(lmr.rx2(\"scale\"), copy=True)\n        residuals = np.array(lmr.rx2(\"residuals\"), copy=True)\n        stderr = np.array(lmr.rx2(\"cov\"), copy=True)[0][0]\n        return betas, residuals, stderr, scale\n    \n    def oR(self, bool=False):\n        '''\n        Specify regression method and run all regression in one/two line(s)\n        '''\n        if self.method == \"OLS\":\n            R0 = np.asarray([self.lsR(i) for i in self.index])\n            self.RA = np.asarray([self.lstsq(i[0], i[1], True) for i in R0])\n            R1 = np.asarray([[self.lsR(i, O=j) for j in self.range] for i in self.index])\n            self.RB = np.asarray([[self.lstsq(i[j][0], i[j][1]) for j in self.range] for i in R1] )           \n        elif self.method == \"MM\":\n            self.RA = np.asarray([self.MM(i, stderr=True) for i in self.index])\n            self.RB = np.asarray([[self.MM(i, j) for j in self.range] for i in self.index])\n        elif self.method == \"LmRob\":\n            R0 = np.asarray([self.lsR(i) for i in self.index])\n            self.RA = np.asarray([self.lmrob(i[0], i[1]) for i in R0])\n            R1 = np.asarray([[self.lsR(i, O=j) for j in self.range] for i in self.index])\n            self.RB = np.asarray([[self.lmrob(i[j][0], i[j][1]) for j in self.range] for i in R1])          \n            \n        if bool:\n            return self.RA.T[0], self.RB.T[0]\n        \n    def LSR(self):\n        self.oR()\n        ''' \n        Calculate Local Sensitivity \n        '''\n        if \"F1\" or \"F2\" in self.figure:\n            LS = np.asarray([[max([abs(self.RB[i][j][0][1] - self.RA[i][0][1]) for j in self.range]), \n                  np.argmax([abs(self.RB[i][j][0][1] - self.RA[i][0][1]) for j in self.range])]\n                  for i in self.index])\n            OSE = np.multiply(self.N, np.transpose(LS)[0])\n            MOSE = max(OSE)\n            if \"F1\" in self.figure:\n                self.MOSE = np.around(max(LS.T[0] * 0.25), 3)\n                self.k = np.argmax(LS.T[0] * 0.25)\n                self.j = LS.T[1][self.k].astype(int)\n                self.cell = str(int(self.index[self.k]))\n            elif \"F2\" in self.figure:\n                self.fD = np.vstack((np.asarray(self.D.N), LS.T[0]))\n                self.MOSE = MOSE\n                self.LS = LS.T[0]\n                self.k = np.argmax(OSE)\n        else:\n            MOSE = max(np.multiply(self.N, \n                [max([abs(self.RB[i][j][0][1] - self.RA[i][0][1]) for j in self.range]) for i in self.index]))\n        return np.transpose([[self.RA[i][j] for j in [0, 2]] for i in self.index]), MOSE\n    \n    def noise(self, θseθ, χ):\n        ''' \n        Add Noise to Statistics \n        '''\n        NN = np.asarray(self.D.N).astype(float)\n        S = NN * self.ϵ\n        noise = lambda x: x * math.sqrt(2)\n        \n        nθ = [i[1] for i in θseθ[0]] + χ * noise(self.ω()) / S\n        sen = np.square(θseθ[1]) + 2 * np.square(χ / S)\n        senθ = np.sqrt(sen.astype(float))\n        nsenθ = senθ + χ * noise(self.ω()) / S\n        nsenθ = np.asarray([\"Sample Size Too Small\" if i==np.inf else i for i in nsenθ])\n        nN = self.D.N * (1 + noise(self.ω()) / S)\n        return nθ, nsenθ, nN\n    \n    def __call__(self):\n        ''' \n        Release Noise Infused Statistics \n        '''\n        m1, m2 = self.LSR()\n        nθ, nsenθ, nN = self.noise(m1, m2)\n        return nθ, nsenθ, nN\n    \n    def plot(self, I=None, O=None):\n        fig, ax = plt.subplots(figsize = (8,6))\n        if \"F1\" in self.figure:\n            self.LSR()\n            # Relevant Data\n            fD = np.vstack((self.D.X[self.k], self.D.Y[self.k]))\n            fo = np.vstack((self.Outlier[self.j][0], self.Outlier[self.j][1]))\n            fLA = lambda λ: self.RA[self.k][0][1]*λ + self.RA[self.k][0][0]\n            fLB = lambda λ: self.RB[self.k][self.j][0][1]*λ + self.RB[self.k][self.j][0][0]\n            \n            # Figure 1 - Effect of Outlier on Regression + MOSE line\n            ax.scatter(fD[0], fD[1])\n            ax.scatter(fo[0], fo[1])\n            l = np.linspace(0, 1, 5)\n            ax.plot(l, fLA(l), label=\"%s-Estimate in Actual Data\"%self.method)\n            ax.plot(l, fLB(l), linestyle = 'dashed', label=\"%s-Estimate with Outlier\"%self.method)\n            # LS line at 25th pctile\n            p25 = [0.25, 0.25]\n            yp25 = [fLA(p25[0]), fLB(p25[1])]\n            ax.plot(p25, yp25, color='black')\n            \n            ax.set_title(\"FIGURE 1: Calculation of Local Sensitivity $= %s$\"%str(self.MOSE), pad=35)\n            ax.set_ylabel(\"Child's Income Rank for Tract %s\"%self.cell)\n            ax.set_xlabel(\"Parent's Income Rank for Tract %s\"%self.cell)\n            \n            plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=2, mode=\"expand\", borderaxespad=0.)\n            ax.set_xlim(-0.025, 1.025)\n            ax.set_ylim(-0.025, 1.025)\n            \n        if \"F2\" in self.figure:\n            self.LSR()\n            fLA = lambda λ: self.MOSE / λ\n            ax.scatter(self.fD[0], self.fD[1])\n            xmin = min(self.D.N)\n            xmax = max(self.D.N)\n            l = [xmin, xmax]\n            ax.plot(l, fLA(l), label=\"$MOSE = χ\\,/\\,N = %s\\,/\\,N $\"%str(np.around(self.MOSE, 2)), color='black')\n            ax.scatter(self.fD[0][self.k], self.fD[1][self.k], label=\"Tract %d\"%self.k)\n            ax.set_title(\"FIGURE 2: Maximum Observed Sensitivity Envelope for %s\"%self.method, pad=35)\n            ax.set_ylabel(\"Local Sensitivity of $̂β_1$ Estimates\")\n            ax.set_xlabel(\"Number of Individuals in Tract\")\n            \n            ax.set_xscale('log')\n            ax.set_yscale('log')\n            \n            ax.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=2, mode=\"expand\", borderaxespad=0.)\n            ax.set_xlim(xmin *0.9, xmax * 1.1)\n            ax.set_ylim(min(self.LS)*0.9, self.MOSE/(xmin *0.9))\n            \n        if \"F3\" in self.figure:\n            # Figure 3\n            l = np.linspace(0, 1, 5)\n            if O is not None:\n                ax.scatter(self.Outlier[O][0], self.Outlier[O][1])\n                ao, bo = self.lsR(I, O)\n                if self.method == \"OLS\":\n                    co, do, eo = self.lstsq(ao, bo)\n                if self.method == \"MM\":\n                    co, do, eo, fo = self.MM(I, O)\n                if self.method == \"LmRob\":\n                    co, do, eo, fo = self.lmrob(ao, bo)\n                lineo = lambda x: co[1]*x + co[0]\n                ax.plot(l, lineo(l))\n            ax.scatter(self.D.X[I], self.D.Y[I])\n            a, b = self.lsR(I)\n            if self.method == \"OLS\":\n                c, d, e = self.lstsq(a, b)\n            if self.method == \"MM\":\n                c, d, e, f = self.MM(I)\n            if self.method == \"LmRob\":\n                c, d, e, f = self.lmrob(a, b)\n            line = lambda x: c[1]*x + c[0]\n            ax.plot(l, line(l))\n            ax.set_xlim(-0.025, 1.025)\n            ax.set_ylim(-0.025, 1.025)\n\n        ax.grid(which='minor', color='w', alpha=0.3)\n        plt.show()",
      "metadata": {}
    }
  ]
}